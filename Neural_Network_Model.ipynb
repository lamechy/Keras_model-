{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing all the libraries we'll be making use of.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "%run ModelTrain_Data.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our sequential model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2100 samples, validate on 101 samples\n",
      "Epoch 1/30\n",
      "2100/2100 - 4s - loss: 0.6556 - accuracy: 0.5557 - val_loss: 0.6407 - val_accuracy: 0.5842\n",
      "Epoch 2/30\n",
      "2100/2100 - 2s - loss: 0.6264 - accuracy: 0.6205 - val_loss: 0.6104 - val_accuracy: 0.6337\n",
      "Epoch 3/30\n",
      "2100/2100 - 2s - loss: 0.5993 - accuracy: 0.6781 - val_loss: 0.5824 - val_accuracy: 0.7129\n",
      "Epoch 4/30\n",
      "2100/2100 - 1s - loss: 0.5736 - accuracy: 0.7162 - val_loss: 0.5545 - val_accuracy: 0.7525\n",
      "Epoch 5/30\n",
      "2100/2100 - 2s - loss: 0.5482 - accuracy: 0.7462 - val_loss: 0.5267 - val_accuracy: 0.7723\n",
      "Epoch 6/30\n",
      "2100/2100 - 1s - loss: 0.5230 - accuracy: 0.7829 - val_loss: 0.4981 - val_accuracy: 0.7822\n",
      "Epoch 7/30\n",
      "2100/2100 - 1s - loss: 0.4979 - accuracy: 0.8024 - val_loss: 0.4700 - val_accuracy: 0.8317\n",
      "Epoch 8/30\n",
      "2100/2100 - 2s - loss: 0.4737 - accuracy: 0.8290 - val_loss: 0.4433 - val_accuracy: 0.8515\n",
      "Epoch 9/30\n",
      "2100/2100 - 2s - loss: 0.4504 - accuracy: 0.8452 - val_loss: 0.4175 - val_accuracy: 0.8614\n",
      "Epoch 10/30\n",
      "2100/2100 - 2s - loss: 0.4283 - accuracy: 0.8614 - val_loss: 0.3917 - val_accuracy: 0.9208\n",
      "Epoch 11/30\n",
      "2100/2100 - 1s - loss: 0.4077 - accuracy: 0.8781 - val_loss: 0.3685 - val_accuracy: 0.9208\n",
      "Epoch 12/30\n",
      "2100/2100 - 2s - loss: 0.3888 - accuracy: 0.8886 - val_loss: 0.3473 - val_accuracy: 0.9208\n",
      "Epoch 13/30\n",
      "2100/2100 - 2s - loss: 0.3715 - accuracy: 0.8971 - val_loss: 0.3283 - val_accuracy: 0.9307\n",
      "Epoch 14/30\n",
      "2100/2100 - 2s - loss: 0.3562 - accuracy: 0.9048 - val_loss: 0.3110 - val_accuracy: 0.9307\n",
      "Epoch 15/30\n",
      "2100/2100 - 2s - loss: 0.3426 - accuracy: 0.9090 - val_loss: 0.2957 - val_accuracy: 0.9406\n",
      "Epoch 16/30\n",
      "2100/2100 - 2s - loss: 0.3307 - accuracy: 0.9186 - val_loss: 0.2814 - val_accuracy: 0.9406\n",
      "Epoch 17/30\n",
      "2100/2100 - 2s - loss: 0.3206 - accuracy: 0.9238 - val_loss: 0.2692 - val_accuracy: 0.9505\n",
      "Epoch 18/30\n",
      "2100/2100 - 2s - loss: 0.3120 - accuracy: 0.9295 - val_loss: 0.2597 - val_accuracy: 0.9505\n",
      "Epoch 19/30\n",
      "2100/2100 - 2s - loss: 0.3044 - accuracy: 0.9295 - val_loss: 0.2506 - val_accuracy: 0.9505\n",
      "Epoch 20/30\n",
      "2100/2100 - 2s - loss: 0.2979 - accuracy: 0.9329 - val_loss: 0.2450 - val_accuracy: 0.9505\n",
      "Epoch 21/30\n",
      "2100/2100 - 2s - loss: 0.2925 - accuracy: 0.9333 - val_loss: 0.2381 - val_accuracy: 0.9505\n",
      "Epoch 22/30\n",
      "2100/2100 - 2s - loss: 0.2879 - accuracy: 0.9310 - val_loss: 0.2308 - val_accuracy: 0.9505\n",
      "Epoch 23/30\n",
      "2100/2100 - 2s - loss: 0.2840 - accuracy: 0.9357 - val_loss: 0.2263 - val_accuracy: 0.9505\n",
      "Epoch 24/30\n",
      "2100/2100 - 1s - loss: 0.2807 - accuracy: 0.9381 - val_loss: 0.2225 - val_accuracy: 0.9505\n",
      "Epoch 25/30\n",
      "2100/2100 - 1s - loss: 0.2779 - accuracy: 0.9390 - val_loss: 0.2200 - val_accuracy: 0.9505\n",
      "Epoch 26/30\n",
      "2100/2100 - 1s - loss: 0.2753 - accuracy: 0.9381 - val_loss: 0.2153 - val_accuracy: 0.9703\n",
      "Epoch 27/30\n",
      "2100/2100 - 1s - loss: 0.2734 - accuracy: 0.9433 - val_loss: 0.2142 - val_accuracy: 0.9505\n",
      "Epoch 28/30\n",
      "2100/2100 - 2s - loss: 0.2715 - accuracy: 0.9386 - val_loss: 0.2110 - val_accuracy: 0.9703\n",
      "Epoch 29/30\n",
      "2100/2100 - 2s - loss: 0.2701 - accuracy: 0.9457 - val_loss: 0.2101 - val_accuracy: 0.9703\n",
      "Epoch 30/30\n",
      "2100/2100 - 2s - loss: 0.2689 - accuracy: 0.9419 - val_loss: 0.2063 - val_accuracy: 0.9703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f75df68c690>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(units=16, input_shape=(1,), activation='relu'),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=2, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x=scaled_train_samples, y=train_labels, batch_size=10, validation_data=valid_set, epochs=30, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating The Test Set to be  used by our already trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_labels = []\n",
    "test_samples = []\n",
    "\n",
    "for i in range(10):\n",
    "    #The 5% of younger individuals who did experience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(1)\n",
    "    \n",
    "    \n",
    "    #The 5% of older individuals who didn't experience side effects\n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(0)\n",
    "    \n",
    "for i in range(200):\n",
    "    #The 95% of younger individuals who didn't experince side effects \n",
    "    random_younger = randint(13,64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(0)\n",
    "    \n",
    "    #The 95% of older individuals who experienced side effects\n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(1)\n",
    "    \n",
    "    \n",
    "test_labels = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)\n",
    "test_labels, test_samples = shuffle(test_labels, test_samples)\n",
    "\n",
    "#scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_test_samples = scaler.fit_transform(test_samples.reshape(-1,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalutating the test set \n",
    "\n",
    "To get predictions from the model forthe test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.061238155: 1, 0.9387619: 1}\n",
      "{0.068982065: 1, 0.9310179: 1}\n",
      "{0.06535037: 1, 0.9346496: 1}\n",
      "{0.0440579: 1, 0.9559421: 1}\n",
      "{0.009441714: 1, 0.99055827: 1}\n",
      "{0.010752535: 1, 0.9892475: 1}\n",
      "{0.034229144: 1, 0.96577084: 1}\n",
      "{0.020529246: 1, 0.9794708: 1}\n",
      "{0.030143555: 1, 0.9698564: 1}\n",
      "{0.06397151: 1, 0.93602854: 1}\n",
      "{0.14630125: 1, 0.8536988: 1}\n",
      "{0.1634769: 1, 0.8365231: 1}\n",
      "{0.10359575: 1, 0.89640427: 1}\n",
      "{0.22468008: 1, 0.7753199: 1}\n",
      "{0.06776228: 1, 0.93223774: 1}\n",
      "{0.27012637: 1, 0.7298736: 1}\n",
      "{0.27369782: 1, 0.7263022: 1}\n",
      "{0.06852573: 1, 0.9314742: 1}\n",
      "{0.44867837: 1, 0.55132157: 1}\n",
      "{0.102655746: 1, 0.89734423: 1}\n",
      "{0.04993234: 1, 0.95006764: 1}\n",
      "{0.08161892: 1, 0.91838115: 1}\n",
      "{0.08161892: 1, 0.91838115: 1}\n",
      "{0.07409686: 1, 0.92590314: 1}\n",
      "{0.068982065: 1, 0.9310179: 1}\n",
      "{0.3892113: 1, 0.61078864: 1}\n",
      "{0.06852573: 1, 0.9314742: 1}\n",
      "{0.020529246: 1, 0.9794708: 1}\n",
      "{0.3288733: 1, 0.67112666: 1}\n",
      "{0.0440579: 1, 0.9559421: 1}\n",
      "{0.008289356: 1, 0.99171066: 1}\n",
      "{0.1822385: 1, 0.81776154: 1}\n",
      "{0.061238155: 1, 0.9387619: 1}\n",
      "{0.07279987: 1, 0.9272001: 1}\n",
      "{0.060225032: 1, 0.939775: 1}\n",
      "{0.015862338: 1, 0.98413765: 1}\n",
      "{0.07279987: 1, 0.9272001: 1}\n",
      "{0.2026318: 1, 0.79736817: 1}\n",
      "{0.008289356: 1, 0.99171066: 1}\n",
      "{0.060284365: 1, 0.9397156: 1}\n",
      "{0.068982065: 1, 0.9310179: 1}\n",
      "{0.06397151: 1, 0.93602854: 1}\n",
      "{0.41645566: 1, 0.5835444: 1}\n",
      "{0.061933: 1, 0.938067: 1}\n",
      "{0.41645566: 1, 0.5835444: 1}\n",
      "{0.06038487: 1, 0.9396151: 1}\n",
      "{0.067006685: 1, 0.9329933: 1}\n",
      "{0.10359575: 1, 0.89640427: 1}\n",
      "{0.06929719: 1, 0.93070287: 1}\n",
      "{0.07007663: 1, 0.92992336: 1}\n",
      "{0.08161892: 1, 0.91838115: 1}\n",
      "{0.077586815: 1, 0.9224132: 1}\n",
      "{0.008289356: 1, 0.99171066: 1}\n",
      "{0.03884633: 1, 0.9611537: 1}\n",
      "{0.22468008: 1, 0.7753199: 1}\n",
      "{0.27012637: 1, 0.7298736: 1}\n",
      "{0.062490046: 1, 0.9375099: 1}\n",
      "{0.060225032: 1, 0.939775: 1}\n",
      "{0.026532203: 1, 0.9734678: 1}\n",
      "{0.10359575: 1, 0.89640427: 1}\n",
      "{0.1634769: 1, 0.8365231: 1}\n",
      "{0.1634769: 1, 0.8365231: 1}\n",
      "{0.48584273: 1, 0.5141573: 1}\n",
      "{0.018048305: 1, 0.9819517: 1}\n",
      "{0.06625894: 1, 0.93374103: 1}\n",
      "{0.010752535: 1, 0.9892475: 1}\n",
      "{0.060593072: 1, 0.93940693: 1}\n",
      "{0.060284365: 1, 0.9397156: 1}\n",
      "{0.1822385: 1, 0.81776154: 1}\n",
      "{0.30056405: 1, 0.69943595: 1}\n",
      "{0.060284365: 1, 0.9397156: 1}\n",
      "{0.056543697: 1, 0.94345635: 1}\n",
      "{0.02334312: 1, 0.97665685: 1}\n",
      "{0.056543697: 1, 0.94345635: 1}\n",
      "{0.034229144: 1, 0.96577084: 1}\n",
      "{0.27369782: 1, 0.7263022: 1}\n",
      "{0.30056405: 1, 0.69943595: 1}\n",
      "{0.060392987: 1, 0.93960696: 1}\n",
      "{0.1634769: 1, 0.8365231: 1}\n",
      "{0.45314482: 1, 0.54685515: 1}\n",
      "{0.010752535: 1, 0.9892475: 1}\n",
      "{0.077586815: 1, 0.9224132: 1}\n",
      "{0.020529246: 1, 0.9794708: 1}\n",
      "{0.02334312: 1, 0.97665685: 1}\n",
      "{0.060545094: 1, 0.9394549: 1}\n",
      "{0.3288733: 1, 0.67112666: 1}\n",
      "{0.020529246: 1, 0.9794708: 1}\n",
      "{0.02334312: 1, 0.97665685: 1}\n",
      "{0.41645566: 1, 0.5835444: 1}\n",
      "{0.14630125: 1, 0.8536988: 1}\n",
      "{0.29678532: 1, 0.70321465: 1}\n",
      "{0.14630125: 1, 0.8536988: 1}\n",
      "{0.07279987: 1, 0.9272001: 1}\n",
      "{0.2026318: 1, 0.79736817: 1}\n",
      "{0.14406244: 1, 0.85593754: 1}\n",
      "{0.3892113: 1, 0.61078864: 1}\n",
      "{0.07327622: 1, 0.9267238: 1}\n",
      "{0.020529246: 1, 0.9794708: 1}\n",
      "{0.06334485: 1, 0.9366552: 1}\n",
      "{0.3892113: 1, 0.61078864: 1}\n",
      "{0.06406198: 1, 0.9359381: 1}\n",
      "{0.48584273: 1, 0.5141573: 1}\n",
      "{0.13064829: 1, 0.8693518: 1}\n",
      "{0.06535037: 1, 0.9346496: 1}\n",
      "{0.41645566: 1, 0.5835444: 1}\n",
      "{0.3288733: 1, 0.67112666: 1}\n",
      "{0.06108444: 1, 0.9389156: 1}\n",
      "{0.060545094: 1, 0.9394549: 1}\n",
      "{0.07327622: 1, 0.9267238: 1}\n",
      "{0.067006685: 1, 0.9329933: 1}\n",
      "{0.06263522: 1, 0.9373648: 1}\n",
      "{0.08468537: 1, 0.9153147: 1}\n",
      "{0.10359575: 1, 0.89640427: 1}\n",
      "{0.056543697: 1, 0.94345635: 1}\n",
      "{0.015862338: 1, 0.98413765: 1}\n",
      "{0.0440579: 1, 0.9559421: 1}\n",
      "{0.009441714: 1, 0.99055827: 1}\n",
      "{0.07246392: 1, 0.9275361: 1}\n",
      "{0.062490046: 1, 0.9375099: 1}\n",
      "{0.061238155: 1, 0.9387619: 1}\n",
      "{0.061238155: 1, 0.9387619: 1}\n",
      "{0.010752535: 1, 0.9892475: 1}\n",
      "{0.013937361: 1, 0.9860626: 1}\n",
      "{0.010752535: 1, 0.9892475: 1}\n",
      "{0.13064829: 1, 0.8693518: 1}\n",
      "{0.06334485: 1, 0.9366552: 1}\n",
      "{0.06397151: 1, 0.93602854: 1}\n",
      "{0.06108444: 1, 0.9389156: 1}\n",
      "{0.056543697: 1, 0.94345635: 1}\n",
      "{0.06535037: 1, 0.9346496: 1}\n",
      "{0.060545094: 1, 0.9394549: 1}\n",
      "{0.3892113: 1, 0.61078864: 1}\n",
      "{0.27369782: 1, 0.7263022: 1}\n",
      "{0.02334312: 1, 0.97665685: 1}\n",
      "{0.02334312: 1, 0.97665685: 1}\n",
      "{0.04993234: 1, 0.95006764: 1}\n",
      "{0.02334312: 1, 0.97665685: 1}\n",
      "{0.08161892: 1, 0.91838115: 1}\n",
      "{0.102655746: 1, 0.89734423: 1}\n",
      "{0.06263522: 1, 0.9373648: 1}\n",
      "{0.06406198: 1, 0.9359381: 1}\n",
      "{0.07165995: 1, 0.9283401: 1}\n",
      "{0.06852573: 1, 0.9314742: 1}\n",
      "{0.14630125: 1, 0.8536988: 1}\n",
      "{0.4208462: 1, 0.57915384: 1}\n",
      "{0.30056405: 1, 0.69943595: 1}\n",
      "{0.1634769: 1, 0.8365231: 1}\n",
      "{0.324904: 1, 0.67509604: 1}\n",
      "{0.068982065: 1, 0.9310179: 1}\n",
      "{0.015862338: 1, 0.98413765: 1}\n",
      "{0.1634769: 1, 0.8365231: 1}\n",
      "{0.4813377: 1, 0.51866233: 1}\n",
      "{0.07327622: 1, 0.9267238: 1}\n",
      "{0.026532203: 1, 0.9734678: 1}\n",
      "{0.07007663: 1, 0.92992336: 1}\n",
      "{0.04993234: 1, 0.95006764: 1}\n",
      "{0.07086422: 1, 0.92913586: 1}\n",
      "{0.24502772: 1, 0.75497234: 1}\n",
      "{0.0440579: 1, 0.9559421: 1}\n",
      "{0.3584824: 1, 0.64151764: 1}\n",
      "{0.020529246: 1, 0.9794708: 1}\n",
      "{0.44867837: 1, 0.55132157: 1}\n",
      "{0.009441714: 1, 0.99055827: 1}\n",
      "{0.056543697: 1, 0.94345635: 1}\n",
      "{0.3584824: 1, 0.64151764: 1}\n",
      "{0.056543697: 1, 0.94345635: 1}\n",
      "{0.29678532: 1, 0.70321465: 1}\n",
      "{0.10359575: 1, 0.89640427: 1}\n",
      "{0.062490046: 1, 0.9375099: 1}\n",
      "{0.30056405: 1, 0.69943595: 1}\n",
      "{0.012243096: 1, 0.98775685: 1}\n",
      "{0.1822385: 1, 0.81776154: 1}\n",
      "{0.45314482: 1, 0.54685515: 1}\n",
      "{0.14630125: 1, 0.8536988: 1}\n",
      "{0.06038487: 1, 0.9396151: 1}\n",
      "{0.3288733: 1, 0.67112666: 1}\n",
      "{0.116441704: 1, 0.8835584: 1}\n",
      "{0.009441714: 1, 0.99055827: 1}\n",
      "{0.009441714: 1, 0.99055827: 1}\n",
      "{0.061933: 1, 0.938067: 1}\n",
      "{0.06038487: 1, 0.9396151: 1}\n",
      "{0.060545094: 1, 0.9394549: 1}\n",
      "{0.060392987: 1, 0.93960696: 1}\n",
      "{0.062490046: 1, 0.9375099: 1}\n",
      "{0.27369782: 1, 0.7263022: 1}\n",
      "{0.07327622: 1, 0.9267238: 1}\n",
      "{0.020529246: 1, 0.9794708: 1}\n",
      "{0.061238155: 1, 0.9387619: 1}\n",
      "{0.067006685: 1, 0.9329933: 1}\n",
      "{0.14630125: 1, 0.8536988: 1}\n",
      "{0.17956547: 1, 0.8204345: 1}\n",
      "{0.065518945: 1, 0.934481: 1}\n",
      "{0.06852573: 1, 0.9314742: 1}\n",
      "{0.06406198: 1, 0.9359381: 1}\n",
      "{0.07327622: 1, 0.9267238: 1}\n",
      "{0.08161892: 1, 0.91838115: 1}\n",
      "{0.10359575: 1, 0.89640427: 1}\n",
      "{0.060752314: 1, 0.9392477: 1}\n",
      "{0.10359575: 1, 0.89640427: 1}\n",
      "{0.116441704: 1, 0.8835584: 1}\n",
      "{0.013937361: 1, 0.9860626: 1}\n",
      "{0.06625894: 1, 0.93374103: 1}\n",
      "{0.07279987: 1, 0.9272001: 1}\n",
      "{0.067006685: 1, 0.9329933: 1}\n",
      "{0.3543444: 1, 0.6456556: 1}\n",
      "{0.12861295: 1, 0.87138706: 1}\n",
      "{0.060392987: 1, 0.93960696: 1}\n",
      "{0.06478664: 1, 0.9352133: 1}\n",
      "{0.07007663: 1, 0.92992336: 1}\n",
      "{0.14406244: 1, 0.85593754: 1}\n",
      "{0.48584273: 1, 0.5141573: 1}\n",
      "{0.07492599: 1, 0.92507404: 1}\n",
      "{0.061933: 1, 0.938067: 1}\n",
      "{0.14630125: 1, 0.8536988: 1}\n",
      "{0.06852573: 1, 0.9314742: 1}\n",
      "{0.077586815: 1, 0.9224132: 1}\n",
      "{0.060593072: 1, 0.93940693: 1}\n",
      "{0.4813377: 1, 0.51866233: 1}\n",
      "{0.41645566: 1, 0.5835444: 1}\n",
      "{0.060392987: 1, 0.93960696: 1}\n",
      "{0.06038487: 1, 0.9396151: 1}\n",
      "{0.07246392: 1, 0.9275361: 1}\n",
      "{0.018048305: 1, 0.9819517: 1}\n",
      "{0.19973268: 1, 0.80026734: 1}\n",
      "{0.07327622: 1, 0.9267238: 1}\n",
      "{0.4208462: 1, 0.57915384: 1}\n",
      "{0.1634769: 1, 0.8365231: 1}\n",
      "{0.12861295: 1, 0.87138706: 1}\n",
      "{0.020529246: 1, 0.9794708: 1}\n",
      "{0.116441704: 1, 0.8835584: 1}\n",
      "{0.009441714: 1, 0.99055827: 1}\n",
      "{0.060225032: 1, 0.939775: 1}\n",
      "{0.24502772: 1, 0.75497234: 1}\n",
      "{0.013937361: 1, 0.9860626: 1}\n",
      "{0.067006685: 1, 0.9329933: 1}\n",
      "{0.3892113: 1, 0.61078864: 1}\n",
      "{0.008289356: 1, 0.99171066: 1}\n",
      "{0.3543444: 1, 0.6456556: 1}\n",
      "{0.06406198: 1, 0.9359381: 1}\n",
      "{0.060593072: 1, 0.93940693: 1}\n",
      "{0.3288733: 1, 0.67112666: 1}\n",
      "{0.2026318: 1, 0.79736817: 1}\n",
      "{0.4813377: 1, 0.51866233: 1}\n",
      "{0.06776228: 1, 0.93223774: 1}\n",
      "{0.08161892: 1, 0.91838115: 1}\n",
      "{0.2026318: 1, 0.79736817: 1}\n",
      "{0.12861295: 1, 0.87138706: 1}\n",
      "{0.3543444: 1, 0.6456556: 1}\n",
      "{0.41645566: 1, 0.5835444: 1}\n",
      "{0.12861295: 1, 0.87138706: 1}\n",
      "{0.24838027: 1, 0.75161976: 1}\n",
      "{0.116441704: 1, 0.8835584: 1}\n",
      "{0.03884633: 1, 0.9611537: 1}\n",
      "{0.010752535: 1, 0.9892475: 1}\n",
      "{0.11459844: 1, 0.8854016: 1}\n",
      "{0.3288733: 1, 0.67112666: 1}\n",
      "{0.07409686: 1, 0.92590314: 1}\n",
      "{0.02334312: 1, 0.97665685: 1}\n",
      "{0.068982065: 1, 0.9310179: 1}\n",
      "{0.19973268: 1, 0.80026734: 1}\n",
      "{0.061238155: 1, 0.9387619: 1}\n",
      "{0.14630125: 1, 0.8536988: 1}\n",
      "{0.008289356: 1, 0.99171066: 1}\n",
      "{0.22155322: 1, 0.7784468: 1}\n",
      "{0.030143555: 1, 0.9698564: 1}\n",
      "{0.22468008: 1, 0.7753199: 1}\n",
      "{0.08468537: 1, 0.9153147: 1}\n",
      "{0.29678532: 1, 0.70321465: 1}\n",
      "{0.010752535: 1, 0.9892475: 1}\n",
      "{0.2026318: 1, 0.79736817: 1}\n",
      "{0.07165995: 1, 0.9283401: 1}\n",
      "{0.45314482: 1, 0.54685515: 1}\n",
      "{0.29678532: 1, 0.70321465: 1}\n",
      "{0.06478664: 1, 0.9352133: 1}\n",
      "{0.3892113: 1, 0.61078864: 1}\n",
      "{0.14630125: 1, 0.8536988: 1}\n",
      "{0.07327622: 1, 0.9267238: 1}\n",
      "{0.41645566: 1, 0.5835444: 1}\n",
      "{0.14406244: 1, 0.85593754: 1}\n",
      "{0.06038487: 1, 0.9396151: 1}\n",
      "{0.22155322: 1, 0.7784468: 1}\n",
      "{0.009441714: 1, 0.99055827: 1}\n",
      "{0.09310707: 1, 0.90689296: 1}\n",
      "{0.116441704: 1, 0.8835584: 1}\n",
      "{0.060225032: 1, 0.939775: 1}\n",
      "{0.08468537: 1, 0.9153147: 1}\n",
      "{0.03884633: 1, 0.9611537: 1}\n",
      "{0.061933: 1, 0.938067: 1}\n",
      "{0.4208462: 1, 0.57915384: 1}\n",
      "{0.10359575: 1, 0.89640427: 1}\n",
      "{0.008289356: 1, 0.99171066: 1}\n",
      "{0.22468008: 1, 0.7753199: 1}\n",
      "{0.06406198: 1, 0.9359381: 1}\n",
      "{0.067006685: 1, 0.9329933: 1}\n",
      "{0.27369782: 1, 0.7263022: 1}\n",
      "{0.07086422: 1, 0.92913586: 1}\n",
      "{0.29678532: 1, 0.70321465: 1}\n",
      "{0.010752535: 1, 0.9892475: 1}\n",
      "{0.3892113: 1, 0.61078864: 1}\n",
      "{0.06852573: 1, 0.9314742: 1}\n",
      "{0.29678532: 1, 0.70321465: 1}\n",
      "{0.4813377: 1, 0.51866233: 1}\n",
      "{0.026532203: 1, 0.9734678: 1}\n",
      "{0.08468537: 1, 0.9153147: 1}\n",
      "{0.2026318: 1, 0.79736817: 1}\n",
      "{0.10359575: 1, 0.89640427: 1}\n",
      "{0.02334312: 1, 0.97665685: 1}\n",
      "{0.10359575: 1, 0.89640427: 1}\n",
      "{0.03884633: 1, 0.9611537: 1}\n",
      "{0.3892113: 1, 0.61078864: 1}\n",
      "{0.16102488: 1, 0.83897513: 1}\n",
      "{0.06776228: 1, 0.93223774: 1}\n",
      "{0.06625894: 1, 0.93374103: 1}\n",
      "{0.06535037: 1, 0.9346496: 1}\n",
      "{0.030143555: 1, 0.9698564: 1}\n",
      "{0.06406198: 1, 0.9359381: 1}\n",
      "{0.1634769: 1, 0.8365231: 1}\n",
      "{0.06263522: 1, 0.9373648: 1}\n",
      "{0.060225032: 1, 0.939775: 1}\n",
      "{0.08161892: 1, 0.91838115: 1}\n",
      "{0.09310707: 1, 0.90689296: 1}\n",
      "{0.077586815: 1, 0.9224132: 1}\n",
      "{0.056543697: 1, 0.94345635: 1}\n",
      "{0.060392987: 1, 0.93960696: 1}\n",
      "{0.02334312: 1, 0.97665685: 1}\n",
      "{0.09310707: 1, 0.90689296: 1}\n",
      "{0.24838027: 1, 0.75161976: 1}\n",
      "{0.3288733: 1, 0.67112666: 1}\n",
      "{0.07327622: 1, 0.9267238: 1}\n",
      "{0.092019424: 1, 0.90798056: 1}\n",
      "{0.08468537: 1, 0.9153147: 1}\n",
      "{0.060392987: 1, 0.93960696: 1}\n",
      "{0.013937361: 1, 0.9860626: 1}\n",
      "{0.09310707: 1, 0.90689296: 1}\n",
      "{0.018048305: 1, 0.9819517: 1}\n",
      "{0.45314482: 1, 0.54685515: 1}\n",
      "{0.06776228: 1, 0.93223774: 1}\n",
      "{0.45314482: 1, 0.54685515: 1}\n",
      "{0.04993234: 1, 0.95006764: 1}\n",
      "{0.07409686: 1, 0.92590314: 1}\n",
      "{0.1634769: 1, 0.8365231: 1}\n",
      "{0.2026318: 1, 0.79736817: 1}\n",
      "{0.060225032: 1, 0.939775: 1}\n",
      "{0.02334312: 1, 0.97665685: 1}\n",
      "{0.07327622: 1, 0.9267238: 1}\n",
      "{0.19973268: 1, 0.80026734: 1}\n",
      "{0.19973268: 1, 0.80026734: 1}\n",
      "{0.07086422: 1, 0.92913586: 1}\n",
      "{0.092019424: 1, 0.90798056: 1}\n",
      "{0.04993234: 1, 0.95006764: 1}\n",
      "{0.22155322: 1, 0.7784468: 1}\n",
      "{0.10359575: 1, 0.89640427: 1}\n",
      "{0.24838027: 1, 0.75161976: 1}\n",
      "{0.060545094: 1, 0.9394549: 1}\n",
      "{0.067006685: 1, 0.9329933: 1}\n",
      "{0.018048305: 1, 0.9819517: 1}\n",
      "{0.07492599: 1, 0.92507404: 1}\n",
      "{0.07409686: 1, 0.92590314: 1}\n",
      "{0.30056405: 1, 0.69943595: 1}\n",
      "{0.018048305: 1, 0.9819517: 1}\n",
      "{0.009441714: 1, 0.99055827: 1}\n",
      "{0.060225032: 1, 0.939775: 1}\n",
      "{0.060545094: 1, 0.9394549: 1}\n",
      "{0.41645566: 1, 0.5835444: 1}\n",
      "{0.030143555: 1, 0.9698564: 1}\n",
      "{0.12861295: 1, 0.87138706: 1}\n",
      "{0.324904: 1, 0.67509604: 1}\n",
      "{0.06478664: 1, 0.9352133: 1}\n",
      "{0.07409686: 1, 0.92590314: 1}\n",
      "{0.06929719: 1, 0.93070287: 1}\n",
      "{0.060225032: 1, 0.939775: 1}\n",
      "{0.034229144: 1, 0.96577084: 1}\n",
      "{0.3849313: 1, 0.61506873: 1}\n",
      "{0.08161892: 1, 0.91838115: 1}\n",
      "{0.0440579: 1, 0.9559421: 1}\n",
      "{0.018048305: 1, 0.9819517: 1}\n",
      "{0.27369782: 1, 0.7263022: 1}\n",
      "{0.06625894: 1, 0.93374103: 1}\n",
      "{0.02334312: 1, 0.97665685: 1}\n",
      "{0.072300315: 1, 0.9276996: 1}\n",
      "{0.072300315: 1, 0.9276996: 1}\n",
      "{0.07279987: 1, 0.9272001: 1}\n",
      "{0.03884633: 1, 0.9611537: 1}\n",
      "{0.026532203: 1, 0.9734678: 1}\n",
      "{0.45314482: 1, 0.54685515: 1}\n",
      "{0.10359575: 1, 0.89640427: 1}\n",
      "{0.08161892: 1, 0.91838115: 1}\n",
      "{0.07327622: 1, 0.9267238: 1}\n",
      "{0.07409686: 1, 0.92590314: 1}\n",
      "{0.0440579: 1, 0.9559421: 1}\n",
      "{0.07007663: 1, 0.92992336: 1}\n",
      "{0.013937361: 1, 0.9860626: 1}\n",
      "{0.48584273: 1, 0.5141573: 1}\n",
      "{0.008289356: 1, 0.99171066: 1}\n",
      "{0.27012637: 1, 0.7298736: 1}\n",
      "{0.08468537: 1, 0.9153147: 1}\n",
      "{0.077586815: 1, 0.9224132: 1}\n",
      "{0.10359575: 1, 0.89640427: 1}\n",
      "{0.060284365: 1, 0.9397156: 1}\n",
      "{0.015862338: 1, 0.98413765: 1}\n",
      "{0.24838027: 1, 0.75161976: 1}\n",
      "{0.03884633: 1, 0.9611537: 1}\n",
      "{0.018048305: 1, 0.9819517: 1}\n",
      "{0.30056405: 1, 0.69943595: 1}\n",
      "{0.060752314: 1, 0.9392477: 1}\n",
      "{0.14406244: 1, 0.85593754: 1}\n",
      "{0.07279987: 1, 0.9272001: 1}\n",
      "{0.026532203: 1, 0.9734678: 1}\n",
      "{0.092019424: 1, 0.90798056: 1}\n",
      "{0.030143555: 1, 0.9698564: 1}\n",
      "{0.1822385: 1, 0.81776154: 1}\n",
      "{0.06108444: 1, 0.9389156: 1}\n",
      "{0.41645566: 1, 0.5835444: 1}\n",
      "{0.06535037: 1, 0.9346496: 1}\n",
      "{0.077586815: 1, 0.9224132: 1}\n",
      "{0.3849313: 1, 0.61506873: 1}\n",
      "{0.3849313: 1, 0.61506873: 1}\n",
      "{0.0440579: 1, 0.9559421: 1}\n",
      "{0.07007663: 1, 0.92992336: 1}\n",
      "{0.018048305: 1, 0.9819517: 1}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "predictions = model.predict(x=scaled_test_samples,batch_size=10,verbose=0)\n",
    "\n",
    "\n",
    "for i in predictions:\n",
    "    unique, counts = np.unique(i, return_counts=True)\n",
    "    z = dict(zip(unique,counts))\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "rounded_predictions = np.argmax(predictions,axis=-1)\n",
    "\n",
    "for i in rounded_predictions:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
